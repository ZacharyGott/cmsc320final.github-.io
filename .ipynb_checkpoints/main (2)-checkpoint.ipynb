{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584581aa-1f74-42dc-8bb2-faa7beeb8584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import linregress, zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ba0b0a-df15-4243-bbde-3d8f83856881",
   "metadata": {},
   "source": [
    "<h1>Checkpoint 2</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad9e4d7-efb1-4633-a338-3ea0416dd7c8",
   "metadata": {},
   "source": [
    "<h3>Checkpoint 2.a - Data preprocessing</h3>\n",
    "Below you will be able to see what our dataset looks like. It has a lot of different labels like year, make, model, etc. and many different data types between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d396f-9882-41e0-928e-0fc4fff43e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and convert types\n",
    "# (a) Import\n",
    "# (b) Parse\n",
    "\n",
    "car_sales_df = pd.read_csv(\"car_sales_data.csv\")\n",
    "display(car_sales_df.head())\n",
    "display(car_sales_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552c2ef7-9883-4dad-bd67-c84c4be0659b",
   "metadata": {},
   "source": [
    "<h3>Cleaning</h3>\n",
    "In order for our data set to be accurate and useable for our model we need to make sure the data is clean. This involves dealing with the missing\n",
    "entries and converting some data to be in a different format. Below you can see the amount of data that is missing in the dataset by the label. For\n",
    "example the make column is missing 10,301 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53e838f-d996-4e6c-a410-ce066398d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Testing/Cleaning\n",
    "display(car_sales_df.isna().sum())\n",
    "\n",
    "display(car_sales_df[car_sales_df.isna().any(axis=1)].head())\n",
    "\n",
    "car_sales_df = car_sales_df.dropna()\n",
    "\n",
    "# Convert sale date to datetime object\n",
    "\n",
    "car_sales_df[\"saledate\"] = pd.to_datetime(\n",
    "    car_sales_df[\"saledate\"].str.split(\" \\(\").str[0],  # apply split elementwise\n",
    "    format=\"%a %b %d %Y %H:%M:%S GMT%z\",\n",
    "    utc=True\n",
    ")\n",
    "\n",
    "# Cars are released in the previous year\n",
    "car_sales_df[\"year\"] = pd.to_datetime(car_sales_df[\"year\"] - 1, format=\"%Y\", utc=True)\n",
    "\n",
    "display(car_sales_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ee456c-80f5-4ee8-bc88-3fd6676bea47",
   "metadata": {},
   "source": [
    "<h3>Checkpoint 2.b - Basic data exploration and summary statistics</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5001277-4ef3-4d1f-b2c5-3f7c6ee17f2d",
   "metadata": {},
   "source": [
    "**#1 - Sumary Statistics**\n",
    "\n",
    "We will provide descriptive statistics about the dataframe as well as visualizations for the numerical values.\n",
    "\n",
    "From our analysis we can see that there are 472,325 entries in the dataset, representing 53 different manufacturers with a total of 768 models. The most popular car in the dataset is the Nisaan Altima with 16,346 entries.\n",
    "\n",
    "Our visualizations show us that most numerical values are right skewed, except for year of manufacture, which is left-skewed. Price is the most right-skewed metric in the set. We have plotted the 25th, 75th, and 99th percentiles along with the mean to illustrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1860ea8-9340-4f9b-a0d2-8f6e7fc2d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the dataframe\n",
    "car_sales_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482a799-0b22-4928-9793-1c019cf2523f",
   "metadata": {},
   "source": [
    "**Information about the manufacturers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a2b6b-484f-477c-b4ce-88d46af6588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of different manufacturers and their representation in the dataset\n",
    "print(f\"Number of Manufacturers:\")\n",
    "display(len(car_sales_df[\"make\"].unique()))\n",
    "\n",
    "print(\"\\nCount of Models by Manufacturers:\")\n",
    "display(car_sales_df.groupby('make')[\"model\"].unique())\n",
    "\n",
    "print(\"\\nNumber of Unique Car Models:\")\n",
    "display(len(car_sales_df[\"model\"].unique()))\n",
    "\n",
    "print(\"\\nMost Popular Model\")\n",
    "display(car_sales_df.groupby(\"model\")[\"make\"].count().idxmax(), car_sales_df.groupby(\"model\")[\"make\"].count().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16927376-682d-4a9d-9a97-c17f9320fe60",
   "metadata": {},
   "source": [
    "**Creating graphs**\n",
    "<br>The below code creates a few different graphs that represent how skewed the numerical columns are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483d940-eb07-4fba-a18c-e7870a10c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of numerical data\n",
    "car_sales_df[\"year\"].hist()\n",
    "plt.title(\"Histogram of Year of Manufacture\")\n",
    "plt.show()\n",
    "\n",
    "car_sales_df[\"odometer\"].hist()\n",
    "plt.title(\"Histogram of Mileage\")\n",
    "plt.show()\n",
    "\n",
    "# The pretty plot\n",
    "car_sales_df[\"sellingprice\"].hist(bins=int(math.sqrt(car_sales_df[\"sellingprice\"].size)))\n",
    "plt.axvline(car_sales_df[\"sellingprice\"].mean(), label=f\"Average Car Price - {car_sales_df['sellingprice'].mean()}\", color=\"black\")\n",
    "plt.axvline(car_sales_df[\"sellingprice\"].quantile(.25), label=f\"25th percentile - {car_sales_df['sellingprice'].quantile(.25)}\", color=\"orange\")\n",
    "plt.axvline(car_sales_df[\"sellingprice\"].quantile(.75), label=f\"75th percentile - {car_sales_df['sellingprice'].quantile(.75)}\", color=\"green\")\n",
    "plt.axvline(car_sales_df[\"sellingprice\"].quantile(.99), label=f\"99th percentile - {car_sales_df['sellingprice'].quantile(.99)}\", color=\"red\")\n",
    "plt.title(\"Histogram of Car Prices\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f400f3-f9ef-4228-a725-9a903c30bd12",
   "metadata": {},
   "source": [
    "<h4>Conclusion #1 - The Relationship Between Age-at-sale and Selling Price</h4>\n",
    "\n",
    "We examine whether there is a significantly negative linear relationship between the logarithm of age at sale and selling price. To control for potential confounding effects, we stratify the analysis by manufacturer.\n",
    "\n",
    "The logarithmic transformation of age at sale accounts for the right-skewed distribution and the exponential depreciation pattern of car prices. Since the logarithm is a strictly increasing function, the direction of the relationship remains the same as in the original scale.\n",
    "\n",
    "h0 : Average selling price for each manufacturer is not inversely correlated with age at date-of-sale.</br>\n",
    "ha : Average selling price for each manufacturer is inversely correlated with age at date-of-sale.\n",
    "\n",
    "We use an alpha value of 0.01.\n",
    "\n",
    "<u>Results</u>\n",
    "A table of results is shown below, filtered for models with at least 30 samples to satisfy the assumptions of our model. The p_value column represents the p-value from a two-tailed regression test. Because all slopes are negative, we can obtain the one-tailed p-value by dividing by two. Equivalently, we consider a result significant if the reported p-value is less than 0.005. As shown, this holds true for all entries, so for every car model we reject the null hypothesis. This shows that average selling price is inversely correlated with the age of the car at date-of-sale for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06029fb-4165-478e-937a-9a7be9ba4061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start by creating and visualizing our data\n",
    "car_sales_df[\"age_at_sale\"] = car_sales_df[\"saledate\"] - car_sales_df[\"year\"]\n",
    "\n",
    "ages = car_sales_df[\"age_at_sale\"].dt.days / 365.25\n",
    "\n",
    "# Compute statistics\n",
    "mean_val = ages.mean()\n",
    "median_val = ages.median()\n",
    "p25 = ages.quantile(0.25)\n",
    "p75 = ages.quantile(0.75)\n",
    "\n",
    "# Create histogram\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.hist(ages, bins=50, color=\"skyblue\", edgecolor=\"black\", alpha=0.7)\n",
    "\n",
    "# Add vertical lines for key stats\n",
    "ax.axvline(mean_val, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Mean: {mean_val:.1f}\")\n",
    "ax.axvline(median_val, color=\"green\", linestyle=\"-.\", linewidth=2, label=f\"Median: {median_val:.1f}\")\n",
    "ax.axvline(p25, color=\"orange\", linestyle=\":\", linewidth=2, label=f\"25th pct: {p25:.1f}\")\n",
    "ax.axvline(p75, color=\"purple\", linestyle=\":\", linewidth=2, label=f\"75th pct: {p75:.1f}\")\n",
    "\n",
    "# Customize plot\n",
    "ax.set_title(\"Distribution of Car Age at Sale (Years)\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Age at Sale (Years)\", fontsize=12)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=12)\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "ax.legend()\n",
    "\n",
    "# Display summary stats below the chart\n",
    "summary_text = (\n",
    "    f\"Mean = {mean_val:.1f} Years\\n\"\n",
    "    f\"Median = {median_val:.1f} Years\\n\"\n",
    "    f\"25th percentile = {p25:.1f} Years\\n\"\n",
    "    f\"75th percentile = {p75:.1f} Years\"\n",
    ")\n",
    "plt.figtext(0.75, 0.6, summary_text, fontsize=10, bbox=dict(facecolor='white', alpha=0.6))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0e62d6-bbfb-4614-a252-27d1b1fbfab0",
   "metadata": {},
   "source": [
    "<h3>Relationship within manufacturers</h3>\n",
    "To further explore the relationship between the sale price of the car and the time the car was sold, we can look at the different plots for the different manufacturers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09dbf8-8036-415c-a2f9-98b6547de679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different dataframes based on the make of the cars\n",
    "grouped_by_make = car_sales_df.groupby(\"make\")\n",
    "\n",
    "n = len(grouped_by_make)\n",
    "cols = 3  # adjust as you like\n",
    "rows = math.ceil(n / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 4 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "results = []\n",
    "\n",
    "# Create the plot for each make\n",
    "for i, (make, group) in enumerate(grouped_by_make):\n",
    "    group = group[group[\"age_at_sale\"].dt.days != 0]\n",
    "    if group.empty:\n",
    "        continue\n",
    "\n",
    "    # X axis will be the logrithm of how old the car is at sale (days). The Y axis is the selling price of the car\n",
    "    x = group[\"age_at_sale\"].dt.days.apply(math.log)\n",
    "    y = group[\"sellingprice\"]\n",
    "\n",
    "    # Perform linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "    results.append({\n",
    "        \"make\": make,\n",
    "        \"slope\": slope,\n",
    "        \"intercept\": intercept,\n",
    "        \"r_squared\": r_value**2,\n",
    "        \"p_value\": p_value,\n",
    "        \"std_err\": std_err,\n",
    "        \"samples\": y.size\n",
    "    })\n",
    "\n",
    "    # Plot scatter + regression line\n",
    "    ax = axes[i]\n",
    "    ax.scatter(x, y, alpha=0.6)\n",
    "    ax.plot(x, slope * x + intercept, color=\"red\", lw=1.5)\n",
    "    ax.set_title(f\"{make}\", fontsize=10, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"log(Age at Sale, Days)\")\n",
    "    ax.set_ylabel(\"Selling Price\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"Age at Sale vs Selling Price by Make\", fontsize=14, fontweight=\"bold\")\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(\"p_value\").reset_index(drop=True)\n",
    "display(results_df.query(\"samples >= 30\").sort_values(by=\"make\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327b651e-756c-424e-8728-555e135edac8",
   "metadata": {},
   "source": [
    "<h4>Conclusion #2 - Checking for Outliers in Numeric-like Features</h4>\n",
    "\n",
    "We check for outlier values in the following features:</br>\n",
    "odometer, sellingprice, age_at_sale, and year </br>\n",
    "\n",
    "First we visualize our data using a box-and-whiskers plot and histograms (again, since we have introduced new data). Then, we quantitativley classify outliers using 1st and 99th percentile cutoffs, and |z-score| > 3\n",
    "\n",
    "Results are shown below. From them we conclude that odometer, sellingprice, and age_at_sale are all significantly rightly skewed with many outliers at > 3 sigma while no outliers were < 3 sigma. While year was significantly left skewed with the converse being true. The skewedness can also be seen in the histograms with the medians being much further left than the means for odometer, sellingprice, and age_at_sale and much further right for year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be74b16-d804-4fdb-955f-f3d7ac41aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74f8cd8-6b10-4e3e-90ab-afffd9919fe7",
   "metadata": {},
   "source": [
    "<h3>Format the data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d09d3a-7629-4f7f-8e74-f71fcaf7e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns that we want to check the outlier values for\n",
    "numeric_columns = (\"odometer\", \"sellingprice\", \"age_at_sale\", \"year\")\n",
    "\n",
    "# Create a new dataframe using only those columns. Transform the data to be correct (correct days be dividing by days in a year).\n",
    "numeric_df = car_sales_df[[\"odometer\", \"sellingprice\", \"age_at_sale\", \"year\"]].copy()\n",
    "numeric_df[\"age_at_sale\"] = numeric_df[\"age_at_sale\"].dt.days / 365.25\n",
    "numeric_df[\"year\"] = numeric_df[\"year\"].dt.year\n",
    "\n",
    "display(numeric_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec403834-395d-4e83-b7ec-13727d6e0f4b",
   "metadata": {},
   "source": [
    "<h3>Create the plots</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97c0e14-6aef-4b85-a9a2-8ec1f41b35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(numeric_columns)\n",
    "cols = 2  # adjust as you like\n",
    "rows = math.ceil(n / cols) * 2\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 4 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_columns):\n",
    "    curr = numeric_df[col]\n",
    "\n",
    "    ax = axes[2 * i]\n",
    "    ax.boxplot(curr, vert=False)\n",
    "    ax.set_title(f\"Boxplot of {col}\", fontsize=10, fontweight=\"bold\")\n",
    "    ax.set_xlabel(col)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "    ax = axes[2 * i + 1]\n",
    "    bins = int(math.sqrt(len(curr.unique())))\n",
    "    ax.hist(curr, bins=bins, color=\"lightblue\", edgecolor=\"black\", alpha=0.7)\n",
    "    \n",
    "    # Add vertical lines for statistics\n",
    "    ax.axvline(curr.mean(), color=\"black\", linestyle=\"-\", linewidth=1.2,\n",
    "               label=f\"Mean - {curr.mean():.2f}\")\n",
    "    ax.axvline(curr.quantile(0.25), color=\"orange\", linestyle=\"--\", linewidth=1.2,\n",
    "               label=f\"25th percentile - {curr.quantile(0.25):.2f}\")\n",
    "    ax.axvline(curr.quantile(0.75), color=\"green\", linestyle=\"--\", linewidth=1.2,\n",
    "               label=f\"75th percentile - {curr.quantile(0.75):.2f}\")\n",
    "    ax.axvline(curr.quantile(0.99), color=\"red\", linestyle=\"--\", linewidth=1.2,\n",
    "               label=f\"99th percentile - {curr.quantile(0.99):.2f}\")\n",
    "    \n",
    "    # Titles and labels\n",
    "    ax.set_title(f\"Histogram of {col}\", fontsize=10, fontweight=\"bold\")\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(\"Occurrences\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"Age at Sale vs Selling Price by Make\", fontsize=14, fontweight=\"bold\")\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f07f0d-0609-4384-9bf4-9ac694acaf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get outliers based on 0.1 0.99, |z| > 3\n",
    "\n",
    "def num_less_than_1st_percentile(x):\n",
    "    return x[x < x.quantile(0.01)].count()\n",
    "\n",
    "def num_greater_than_99th_percentile(x):\n",
    "    return x[x > x.quantile(0.99)].count()\n",
    "\n",
    "def num_greater_than_3z(x):\n",
    "    return x[zscore(x) > 3].count()\n",
    "\n",
    "def num_less_than_neg3z(x):\n",
    "    return x[zscore(x) < -3].count()\n",
    "\n",
    "aggregated_outliers = numeric_df.agg([\n",
    "    num_less_than_1st_percentile,\n",
    "    num_greater_than_99th_percentile,\n",
    "    num_greater_than_3z,\n",
    "    num_less_than_neg3z\n",
    "])\n",
    "\n",
    "display(aggregated_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0fef32-6505-4f4f-bf89-76ddacdad89f",
   "metadata": {},
   "source": [
    "<h4>Conclusion #3 - Investigating MMR vs Selling Price</h4>\n",
    "\n",
    "The dataset has an interesting attribute, MMR, which they describe as... \n",
    "\"MMR values offer an estimate of the market value of each vehicle, allowing for analysis of market trends and fluctuations.\"\n",
    "\n",
    "In this section, we investigate the relationship between mmr and selling price by examining the difference between them - estimate_diff. It will be defined at sellingprice-mmr, representing how much a car sold for over its estimated value.\n",
    "\n",
    "<u>Results</u>\n",
    "</br>Firstly, we notice that the mean difference is about -$146.55, meaning that on average, cars sell for $146.55 below their \"fair value.\" This makes sense, as car sales are typically seller-initiated. The standard deviation is very wide, about $1,741.\n",
    "\n",
    "Next, we look for outliers by displaying values outside of 3 standard deviations. There are 6792 values outside of this range, representing extreme skewness. This is visualized by the histogram.\n",
    "\n",
    "When plotting our scatter plot with a linear regression line we determine that there is a positive linear relation (p=0) between sellingprice and mmr. This coincides with common sense that more expensive and lower volume markets are less effecient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ede87f-2b7a-4f61-9dc6-977bad2c7ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_df[\"estimate_diff\"] = car_sales_df[\"sellingprice\"] - car_sales_df[\"mmr\"]\n",
    "display(car_sales_df[\"estimate_diff\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3465b910-7a13-47bb-a924-8da0afabdac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(car_sales_df[\"estimate_diff\"][abs(zscore(car_sales_df[\"estimate_diff\"])) > 3].reset_index())\n",
    "car_sales_df[\"estimate_diff\"].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff49a2d-03a0-4434-a967-d55db1e23e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "df = car_sales_df[[\"sellingprice\", \"estimate_diff\"]]\n",
    "x = df[\"sellingprice\"].astype(float).values\n",
    "y = df[\"estimate_diff\"].astype(float).values\n",
    "if df.empty:\n",
    "    raise ValueError(\"No data to plot after dropping NaNs.\")\n",
    "\n",
    "# Fit\n",
    "slope, intercept, r, p, se = linregress(x, y)\n",
    "\n",
    "# Plot scatter + regression line\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.scatter(x, y, alpha=0.6)\n",
    "xs = np.linspace(x.min(), x.max(), 200)\n",
    "ax.plot(xs, slope * xs + intercept, color=\"red\", lw=1.5)\n",
    "\n",
    "ax.set_title(\"Selling Price vs Estimate Diff\", fontsize=10, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Selling Price\")\n",
    "ax.set_ylabel(\"Estimate Diff\")\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "ax.text(\n",
    "    0.02, 0.98,\n",
    "    f\"slope={slope:.3g}  R²={r**2:.3f}  p={p:.2e}  n={len(df)}\",\n",
    "    transform=ax.transAxes, va=\"top\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d23f2b1-d394-4869-9baa-d7eb38a9a907",
   "metadata": {},
   "source": [
    "<h1>Checkpoint 3 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcb01bb",
   "metadata": {},
   "source": [
    "ML model:\n",
    "\n",
    "we want to predict sales price of a car given certain features. Regression problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Model for Car Sales Price Prediction\n",
    "# Based on insights from previous analysis:\n",
    "# 1. MMR is a strong predictor (positive linear relationship)\n",
    "# 2. Age at sale is inversely correlated with price\n",
    "# 3. Need to handle categorical features (make, model, body, transmission, state)\n",
    "# 4. Outliers exist in odometer, sellingprice, age_at_sale\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random_state = 42\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e145072",
   "metadata": {},
   "source": [
    "### Feature Engineering and Data Preparation\n",
    "\n",
    "We'll create features based on our insights:\n",
    "- Use MMR as a key feature (strong predictor)\n",
    "- Calculate age_at_sale (inversely correlated with price)\n",
    "- Handle categorical variables (make, model, body, transmission, state)\n",
    "- Use numerical features: odometer, condition, year\n",
    "- Consider handling outliers for better model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a598b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a clean copy of the dataframe for ML\n",
    "ml_df = car_sales_df.copy()\n",
    "\n",
    "# Ensure age_at_sale is calculated (in case it wasn't in previous cells)\n",
    "if \"age_at_sale\" not in ml_df.columns:\n",
    "    ml_df[\"age_at_sale\"] = ml_df[\"saledate\"] - ml_df[\"year\"]\n",
    "    ml_df[\"age_at_sale_years\"] = ml_df[\"age_at_sale\"].dt.days / 365.25\n",
    "else:\n",
    "    ml_df[\"age_at_sale_years\"] = ml_df[\"age_at_sale\"].dt.days / 365.25\n",
    "\n",
    "\n",
    "ml_df[\"year_numeric\"] = ml_df[\"year\"].dt.year\n",
    "\n",
    "# Create additional features\n",
    "# 1. Log of age (as we found log relationship in analysis)\n",
    "ml_df[\"log_age_at_sale\"] = np.log1p(ml_df[\"age_at_sale_years\"])\n",
    "\n",
    "# 2. Log of odometer (to handle right skewness)\n",
    "ml_df[\"log_odometer\"] = np.log1p(ml_df[\"odometer\"])\n",
    "\n",
    "# 3. Mileage per year (condition indicator)\n",
    "ml_df[\"miles_per_year\"] = ml_df[\"odometer\"] / (ml_df[\"age_at_sale_years\"] + 0.1)  # +0.1 to avoid division by zero\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Dataset shape: {ml_df.shape}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(ml_df.isnull().sum()[ml_df.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "# Numerical features\n",
    "numerical_features = [\n",
    "    'mmr',  # Strong predictor from analysis\n",
    "    'odometer',\n",
    "    'condition',\n",
    "    'age_at_sale_years',\n",
    "    'log_age_at_sale',  # Log transformation based on analysis\n",
    "    'log_odometer',\n",
    "    'miles_per_year',\n",
    "    'year_numeric'\n",
    "]\n",
    "\n",
    "# Categorical features (will be encoded)\n",
    "categorical_features = [\n",
    "    'make',\n",
    "    'model', \n",
    "    'body',\n",
    "    'transmission',\n",
    "    'state'\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target = 'sellingprice'\n",
    "\n",
    "# Check which features exist\n",
    "available_numerical = [f for f in numerical_features if f in ml_df.columns]\n",
    "available_categorical = [f for f in categorical_features if f in ml_df.columns]\n",
    "\n",
    "print(f\"Available numerical features: {available_numerical}\")\n",
    "print(f\"Available categorical features: {available_categorical}\")\n",
    "\n",
    "# Create feature matrix and target\n",
    "# For tree-based models, we can use label encoding for categoricals\n",
    "# For linear models, we'd need one-hot encoding, but we'll use tree-based models primarily\n",
    "\n",
    "# Prepare data - drop rows with missing target\n",
    "ml_df_clean = ml_df.dropna(subset=[target]).copy()\n",
    "\n",
    "print(f\"\\nClean dataset shape: {ml_df_clean.shape}\")\n",
    "print(f\"Target variable statistics:\")\n",
    "print(ml_df_clean[target].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76176d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables using Label Encoding\n",
    "# (Tree-based models can handle label-encoded categoricals well)\n",
    "label_encoders = {}\n",
    "ml_df_encoded = ml_df_clean.copy()\n",
    "\n",
    "for col in available_categorical:\n",
    "    le = LabelEncoder()\n",
    "    ml_df_encoded[col + '_encoded'] = le.fit_transform(ml_df_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"{col}: {len(le.classes_)} unique values\")\n",
    "\n",
    "# Create final feature list\n",
    "feature_columns = available_numerical + [col + '_encoded' for col in available_categorical]\n",
    "\n",
    "# Extract features and target\n",
    "X = ml_df_encoded[feature_columns].copy()\n",
    "y = ml_df_encoded[target].copy()\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Features used: {feature_columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle any remaining missing values (shouldn't be many after cleaning)\n",
    "print(f\"Missing values in X: {X.isnull().sum().sum()}\")\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    # Fill with median for numerical, mode for categorical\n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            if '_encoded' in col:\n",
    "                X[col].fillna(X[col].mode()[0], inplace=True)\n",
    "            else:\n",
    "                X[col].fillna(X[col].median(), inplace=True)\n",
    "\n",
    "# Split data into train and test sets\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"\\nTraining target statistics:\")\n",
    "print(y_train.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0f7e9",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation\n",
    "\n",
    "We'll train two linear regression models:\n",
    "1. **Linear Regression** - Baseline linear model (Ordinary Least Squares)\n",
    "2. **Ridge Regression** - Regularized linear model with L2 penalty (prevents overfitting)\n",
    "\n",
    "**Note:** Logistic Regression is for classification problems (predicting categories), not Linear Regression (predicting continuous values like price). Since we're predicting car sale price (a continuous value), we use Linear and Ridge Regression instead.\n",
    "\n",
    "We'll evaluate using:\n",
    "- R² Score (coefficient of determination)\n",
    "- Mean Absolute Error (MAE)\n",
    "- Root Mean Squared Error (RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa96c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "# Using Linear Regression and Ridge Regression (regularized linear model)\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0, random_state=random_state)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'y_test_pred': y_test_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"Train R²: {train_r2:.4f} | Test R²: {test_r2:.4f}\")\n",
    "    print(f\"Train MAE: ${train_mae:.2f} | Test MAE: ${test_mae:.2f}\")\n",
    "    print(f\"Train RMSE: ${train_rmse:.2f} | Test RMSE: ${test_rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4f2e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Train R²': [results[m]['train_r2'] for m in results.keys()],\n",
    "    'Test R²': [results[m]['test_r2'] for m in results.keys()],\n",
    "    'Train MAE': [results[m]['train_mae'] for m in results.keys()],\n",
    "    'Test MAE': [results[m]['test_mae'] for m in results.keys()],\n",
    "    'Train RMSE': [results[m]['train_rmse'] for m in results.keys()],\n",
    "    'Test RMSE': [results[m]['test_rmse'] for m in results.keys()]\n",
    "})\n",
    "\n",
    "# Sort by Test R² (best first)\n",
    "comparison_df = comparison_df.sort_values('Test R²', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_df.round(4))\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model = results[best_model_name]['model']\n",
    "print(f\"\\n Best Model: {best_model_name}\")\n",
    "print(f\"   Test R²: {comparison_df.iloc[0]['Test R²']:.4f}\")\n",
    "print(f\"   Test MAE: ${comparison_df.iloc[0]['Test MAE']:.2f}\")\n",
    "print(f\"   Test RMSE: ${comparison_df.iloc[0]['Test RMSE']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492884f8",
   "metadata": {},
   "source": [
    "### Model Visualization and Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual for best model\n",
    "best_pred = results[best_model_name]['y_test_pred']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Scatter plot: Predicted vs Actual\n",
    "axes[0].scatter(y_test, best_pred, alpha=0.5, s=10)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Selling Price ($)', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Selling Price ($)', fontsize=12)\n",
    "axes[0].set_title(f'{best_model_name}: Predicted vs Actual', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - best_pred\n",
    "axes[1].scatter(best_pred, residuals, alpha=0.5, s=10)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Selling Price ($)', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals ($)', fontsize=12)\n",
    "axes[1].set_title(f'{best_model_name}: Residual Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics about residuals\n",
    "print(f\"\\nResidual Statistics:\")\n",
    "print(f\"Mean residual: ${residuals.mean():.2f}\")\n",
    "print(f\"Std residual: ${residuals.std():.2f}\")\n",
    "print(f\"95% of predictions within: ±${np.percentile(np.abs(residuals), 95):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba03be5",
   "metadata": {},
   "source": [
    "### Model Insights and Summary\n",
    "\n",
    "**Key Findings:**\n",
    "1. **MMR** is expected to be the strongest predictor (as found in our analysis)\n",
    "2. **Age at sale** (and its log transformation) should be important due to inverse correlation\n",
    "3. **Odometer** and **miles per year** indicate vehicle condition\n",
    "4. **Make and Model** capture brand value and model-specific pricing\n",
    "5. **Body type** and **transmission** affect pricing\n",
    "\n",
    "The best model will be selected based on Test R² score, which measures how well the model explains variance in selling prices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
